import logging
import concurrent.futures
from datetime import datetime, timedelta
from threading import Lock

logger = logging.getLogger(__name__)


class StatisticsHandler:
    """
    Handles operations related to generating statistics about projects,
    reopened bugs, and issue status transitions.
    """

    def __init__(self, core):
        """Initialize with a JiraCore instance"""
        self.core = core

    def get_project_statistics(self, project_key, start_date=None, end_date=None, participant=None, use_threading=True):
        """
        Get project statistics including total issues, status breakdown, etc.
        
        Args:
            project_key: The Jira project key
            start_date: Optional start date for filtering
            end_date: Optional end date for filtering
            participant: Optional participant for filtering
            use_threading: Whether to use multithreading for better performance
        """
        if not self.core.is_configured():
            logger.error("Jira API credentials not configured")
            return {}

        # Create a cache key based on all filter parameters
        cache_key = f"stats_{project_key}_{start_date}_{end_date}_{participant}"
        
        # Check if we have a valid cache entry
        if self.core._is_cache_valid(cache_key) and cache_key in self.core._projects_cache:
            logger.info(f"Using cached statistics for project {project_key} with filters: start_date={start_date}, end_date={end_date}, participant={participant}")
            return self.core._projects_cache[cache_key]
            
        # If we're filtering by participant, check if we have the full project stats cached
        # and can filter them in memory instead of making a new API call
        if participant:
            base_cache_key = f"stats_{project_key}_{start_date}_{end_date}_None"
            if self.core._is_cache_valid(base_cache_key) and base_cache_key in self.core._projects_cache:
                logger.info(f"Using cached base statistics and filtering by participant: {participant}")
                try:
                    # Get the base statistics
                    base_stats = self.core._projects_cache[base_cache_key]
                    
                    # Filter the participants
                    filtered_stats = self._filter_participants_from_cache(base_stats, participant)
                    
                    # Cache the filtered results
                    self.core._set_cache('projects', cache_key, filtered_stats)
                    
                    return filtered_stats
                except Exception as e:
                    logger.error(f"Error filtering cached statistics by participant: {str(e)}")
                    # Continue with normal API call if filtering fails

        try:
            # Base JQL to get all issues in the project
            jql_parts = [f"project = {project_key}"]

            # Add time constraints if provided
            if start_date:
                jql_parts.append(f'updated >= "{start_date}"')
            if end_date:
                jql_parts.append(f'updated <= "{end_date}"')

            # We'll no longer add participant filter to JQL
            # Instead, we'll get all issues and filter in memory
            # This avoids JQL operator compatibility issues

            jql = " AND ".join(jql_parts)

            # OPTIMIZATION: Get all issues in a single query with all needed fields
            # This reduces the number of API calls significantly
            from jira.issues import IssueHandler
            issue_handler = IssueHandler(self.core)

            try:
                all_issues = issue_handler.search_issues(
                    jql,
                    fields="key,summary,status,assignee,reporter,issuetype,priority,created,updated,comment",
                    max_results=500,  # Reasonable limit for statistics
                    use_cache=False,  # Always fetch fresh data
                    expiry=300  # Cache for 5 minutes (not used due to use_cache=False)
                )
            except Exception as e:
                logger.error(f"Error fetching issues for project {project_key} with JQL '{jql}': {str(e)}")
                all_issues = []

            if not all_issues:
                logger.info(f"No issues found in project {project_key}")
                stats = {
                    'total_issues': 0,
                    'completed_tasks_count': 0,
                    'bugs_count': 0,
                    'reopened_bugs_count': 0,
                    'status_counts': {},
                    'issue_types': {},
                    'recent_issues': [],
                    'participants': [],
                    'total_participants': 0,
                    'reopeners': [],
                    'assignee_bug_stats': []
                }
                self.core._projects_cache[cache_key] = stats
                return stats

            # Count issues by status and type
            status_counts = {}
            issue_types = {}
            completed_tasks_count = 0
            bugs_count = 0
            recent_issues = []

            # Process all issues in a single pass (no additional API calls)
            # If participant filter is provided, we'll filter the results in memory after gathering all statistics
            participant_filter = participant
            
            # Process all issues to gather statistics
            for issue in all_issues:
                try:
                    fields = issue.get('fields', {})
                    if not fields:
                        continue
                    # Get status and count
                    status_obj = fields.get('status')
                    if status_obj is None:
                        continue

                    status = status_obj.get('name', 'Unknown')
                    if status in status_counts:
                        status_counts[status] += 1
                    else:
                        status_counts[status] = 1

                    # Check if completed
                    if status.lower() in ['done', 'closed', 'resolved', 'completed']:
                        completed_tasks_count += 1

                    # Get issue type and count
                    issue_type_obj = fields.get('issuetype')
                    if not issue_type_obj:
                        continue

                    issue_type = issue_type_obj.get('name', 'Unknown')
                    if issue_type in issue_types:
                        issue_types[issue_type] += 1
                    else:
                        issue_types[issue_type] = 1

                    # Count bugs
                    if issue_type.lower() == 'bug':
                        bugs_count += 1

                    # Add to recent issues list
                    try:
                        recent_issue = {
                            'key': issue.get('key', 'Unknown'),
                            'summary': fields.get('summary', 'No summary'),
                            'status': status,
                            'type': issue_type
                        }

                        # Safely add assignee
                        assignee_obj = fields.get('assignee')
                        if assignee_obj and isinstance(assignee_obj, dict):
                            recent_issue['assignee'] = assignee_obj.get('displayName')
                        else:
                            recent_issue['assignee'] = None

                        recent_issue['updated'] = fields.get('updated')
                        recent_issues.append(recent_issue)
                    except Exception as e:
                        logger.error(f"Error processing recent issue: {str(e)}")
                        continue
                except Exception as e:
                    logger.error(f"Error processing issue {issue.get('key', 'Unknown')}: {str(e)}")
                    continue

            # Sort and limit recent issues
            try:
                if recent_issues:
                    recent_issues.sort(key=lambda x: x.get('updated', ''), reverse=True)
                    recent_issues = recent_issues[:50]  # Limit to most recent 50
            except Exception as e:
                logger.error(f"Error sorting recent issues: {str(e)}")
                recent_issues = []

            # OPTIMIZATION: Get participants with caching
            # We don't need to recalculate this for every statistics request
            from jira.projects import ProjectHandler
            project_handler = ProjectHandler(self.core)

            try:
                participants = project_handler.get_project_participants(project_key)
                if participants is None:
                    participants = []
            except Exception as e:
                logger.error(f"Error getting participants for project {project_key}: {str(e)}")
                participants = []

            # Get reopened bugs (filtered to bugs only to reduce API calls)
            try:
                reopened_bugs = self.find_reopened_bugs_by_jql(
                    project_key,
                    start_date=start_date,
                    end_date=end_date,
                    participant=participant
                )

                if reopened_bugs is None:
                    reopened_bugs = []
            except Exception as e:
                logger.error(f"Error finding reopened bugs for project {project_key}: {str(e)}")
                reopened_bugs = []

            reopened_bugs_count = len(reopened_bugs) if reopened_bugs else 0

            # NEW: Gather statistics about who reopened bugs
            reopener_stats = {}

            try:
                for bug in reopened_bugs:
                    if not bug:
                        continue  # Skip None values

                    reopener = bug.get('reopen_by', 'Unknown')
                    if reopener in reopener_stats:
                        reopener_stats[reopener] += 1
                    else:
                        reopener_stats[reopener] = 1
            except Exception as e:
                logger.error(f"Error gathering reopener stats: {str(e)}")

            # Sort reopeners by number of reopens (most first)
            try:
                sorted_reopeners = sorted(
                    [(name, count) for name, count in reopener_stats.items()],
                    key=lambda x: x[1],
                    reverse=True
                )
            except Exception as e:
                logger.error(f"Error sorting reopeners: {str(e)}")
                sorted_reopeners = []

            # NEW: Gather statistics about bugs by assignee using multithreading
            assignee_bug_stats = {}
            assignee_bug_stats_lock = Lock()
            
            # Function to process a single issue for bug statistics
            def process_issue_for_bug_stats(issue):
                if not issue:
                    return None
                    
                fields = issue.get('fields', {})
                if not fields:
                    return None
                    
                issue_type_obj = fields.get('issuetype')
                if not issue_type_obj:
                    return None
                    
                issue_type = issue_type_obj.get('name', 'Unknown')
                
                if issue_type and issue_type.lower() == 'bug':
                    assignee_obj = fields.get('assignee')
                    assignee_name = assignee_obj.get('displayName', 'Unassigned') if assignee_obj else 'Unassigned'
                    
                    # Get unique identifier for assignee (accountId or key)
                    assignee_id = None
                    if assignee_obj:
                        assignee_id = assignee_obj.get('accountId') or assignee_obj.get('key')
                    
                    # Use the unique ID as the key if available, otherwise use name
                    assignee_key = assignee_id if assignee_id else assignee_name
                    
                    # Store full assignee data for reference
                    assignee_data = {}
                    if assignee_obj:
                        # Store all available avatar URLs to ensure we have options for identification
                        avatar_urls = assignee_obj.get('avatarUrls', {})
                        
                        assignee_data = {
                            'displayName': assignee_obj.get('displayName', 'Unknown'),
                            'key': assignee_obj.get('key', ''),
                            'accountId': assignee_obj.get('accountId', ''),
                            'emailAddress': assignee_obj.get('emailAddress', ''),
                            'avatarUrls': avatar_urls,
                            'avatarUrl': next(iter(avatar_urls.values()), '') if avatar_urls else '',
                            'active': assignee_obj.get('active', True),
                            'timeZone': assignee_obj.get('timeZone', ''),
                            'locale': assignee_obj.get('locale', '')
                        }
                    
                    return {
                        'assignee_key': assignee_key,
                        'assignee_name': assignee_name,
                        'assignee_data': assignee_data
                    }
                return None
            
            try:
                # Process issues in parallel if threading is enabled and there are enough issues
                if use_threading and len(all_issues) > 10:
                    # Use a thread pool to process issues in parallel
                    max_workers = min(10, len(all_issues))  # Limit number of threads
                    
                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                        # Submit all issues to the executor
                        future_to_issue = {executor.submit(process_issue_for_bug_stats, issue): issue for issue in all_issues}
                        
                        # Collect results as they complete
                        for future in concurrent.futures.as_completed(future_to_issue):
                            try:
                                result = future.result()
                                if result:
                                    assignee_key = result['assignee_key']
                                    assignee_name = result['assignee_name']
                                    assignee_data = result['assignee_data']
                                    
                                    # Use a lock to safely update the shared dictionary
                                    with assignee_bug_stats_lock:
                                        if assignee_key in assignee_bug_stats:
                                            assignee_bug_stats[assignee_key]['total'] += 1
                                        else:
                                            assignee_bug_stats[assignee_key] = {
                                                'total': 1, 
                                                'reopened': 0,
                                                'name': assignee_name,
                                                'assignee_data': assignee_data
                                            }
                            except Exception as e:
                                logger.error(f"Error processing bug assignee stats in thread: {str(e)}")
                else:
                    # Process sequentially for small datasets or if threading is disabled
                    for issue in all_issues:
                        if not issue:
                            continue

                        fields = issue.get('fields', {})
                        if not fields:
                            continue

                        issue_type_obj = fields.get('issuetype')
                        if not issue_type_obj:
                            continue

                        issue_type = issue_type_obj.get('name', 'Unknown')

                        if issue_type and issue_type.lower() == 'bug':
                            assignee_obj = fields.get('assignee')
                            assignee_name = assignee_obj.get('displayName', 'Unassigned') if assignee_obj else 'Unassigned'
                            
                            # Get unique identifier for assignee (accountId or key)
                            assignee_id = None
                            if assignee_obj:
                                assignee_id = assignee_obj.get('accountId') or assignee_obj.get('key')
                            
                            # Use the unique ID as the key if available, otherwise use name
                            assignee_key = assignee_id if assignee_id else assignee_name
                            
                            if assignee_key in assignee_bug_stats:
                                assignee_bug_stats[assignee_key]['total'] += 1
                            else:
                                # Store full assignee data for reference
                                assignee_data = {}
                                if assignee_obj:
                                    # Store all available avatar URLs to ensure we have options for identification
                                    avatar_urls = assignee_obj.get('avatarUrls', {})
                                    
                                    assignee_data = {
                                        'displayName': assignee_obj.get('displayName', 'Unknown'),
                                        'key': assignee_obj.get('key', ''),
                                        'accountId': assignee_obj.get('accountId', ''),
                                        'emailAddress': assignee_obj.get('emailAddress', ''),
                                        'avatarUrls': avatar_urls,
                                        'avatarUrl': next(iter(avatar_urls.values()), '') if avatar_urls else '',
                                        'active': assignee_obj.get('active', True),
                                        'timeZone': assignee_obj.get('timeZone', ''),
                                        'locale': assignee_obj.get('locale', '')
                                    }
                                
                                assignee_bug_stats[assignee_key] = {
                                    'total': 1, 
                                    'reopened': 0,
                                    'name': assignee_name,
                                    'assignee_data': assignee_data
                                }
            except Exception as e:
                logger.error(f"Error processing bug assignee stats: {str(e)}")

            # Add reopened bugs to assignee stats
            try:
                for bug in reopened_bugs:
                    if not bug:
                        continue  # Skip None values

                    fields = bug.get('fields', {})
                    if not fields:
                        logger.warning(f"Bug {bug.get('key', 'unknown')} has no fields, skipping for assignee stats")
                        continue  # Skip bugs with no fields

                    try:
                        assignee_obj = fields.get('assignee', {})
                        # Extra check for assignee object
                        if not assignee_obj:
                            assignee_name = 'Unassigned'
                            assignee_key = 'Unassigned'
                            assignee_data = {}
                        else:
                            assignee_name = assignee_obj.get('displayName', 'Unassigned')
                            # Get unique identifier for assignee (accountId or key)
                            assignee_id = assignee_obj.get('accountId') or assignee_obj.get('key')
                            # Use the unique ID as the key if available, otherwise use name
                            assignee_key = assignee_id if assignee_id else assignee_name
                            # Store full assignee data for reference
                            # Store all available avatar URLs to ensure we have options for identification
                            avatar_urls = assignee_obj.get('avatarUrls', {})
                            
                            assignee_data = {
                                'displayName': assignee_obj.get('displayName', 'Unknown'),
                                'key': assignee_obj.get('key', ''),
                                'accountId': assignee_obj.get('accountId', ''),
                                'emailAddress': assignee_obj.get('emailAddress', ''),
                                'avatarUrls': avatar_urls,
                                'avatarUrl': next(iter(avatar_urls.values()), '') if avatar_urls else '',
                                'active': assignee_obj.get('active', True),
                                'timeZone': assignee_obj.get('timeZone', ''),
                                'locale': assignee_obj.get('locale', '')
                            }

                        if assignee_key in assignee_bug_stats:
                            assignee_bug_stats[assignee_key]['reopened'] += 1
                        else:
                            assignee_bug_stats[assignee_key] = {
                                'total': 1, 
                                'reopened': 1,
                                'name': assignee_name,
                                'assignee_data': assignee_data
                            }
                    except Exception as e:
                        logger.error(f"Error processing bug for assignee stats: {str(e)}")
                        logger.error(
                            f"Bug data: {bug.get('key')}, fields available: {list(fields.keys()) if fields else 'None'}")
            except Exception as e:
                logger.error(f"Error processing reopened bugs for assignee stats: {str(e)}")

            # Sort assignees by total bugs (most first)
            try:
                # Format the assignee data for the response
                # Use the name field for display but keep the unique identifier as the key
                formatted_assignees = []
                for assignee_key, stats in assignee_bug_stats.items():
                    # Get the display name from the stats if available, otherwise use the key
                    display_name = stats.get('name', assignee_key)
                    formatted_assignees.append([display_name, stats])
                
                # Convert assignee bug stats to include avatar URLs and keys
                enhanced_assignee_bug_stats = []
                for name, stats in assignee_bug_stats.items():
                    # Get assignee data from the stats itself
                    assignee_data = stats.get('assignee_data', {})
                    
                    # Add avatar URL and key to the stats
                    enhanced_stats = stats.copy()
                    enhanced_stats['avatarUrl'] = assignee_data.get('avatarUrl', '')
                    enhanced_stats['key'] = assignee_data.get('key', '')
                    enhanced_stats['email'] = assignee_data.get('emailAddress', '')
                    
                    enhanced_assignee_bug_stats.append((name, enhanced_stats))
                
                # Convert to the format expected by the frontend
                formatted_assignee_bug_stats = []
                for name, stats in enhanced_assignee_bug_stats:
                    formatted_stats = {
                        'name': stats.get('name', name),
                        'total': stats.get('total', 0),
                        'reopened': stats.get('reopened', 0),
                        'key': stats.get('key', ''),
                        'email': stats.get('email', ''),
                        'avatarUrl': stats.get('avatarUrl', '')
                    }
                    formatted_assignee_bug_stats.append(formatted_stats)
                
                # Sort assignee bug stats by total bugs (most first)
                sorted_assignee_bug_stats = sorted(
                    formatted_assignee_bug_stats,
                    key=lambda x: x['total'],
                    reverse=True
                )
            except Exception as e:
                logger.error(f"Error sorting assignee stats: {str(e)}")
                sorted_assignee_bug_stats = []
                sorted_assignees = []

            # Build statistics response
            try:
                statistics = {
                    'total_issues': len(all_issues),
                    'completed_tasks_count': completed_tasks_count,
                    'bugs_count': bugs_count,
                    'reopened_bugs_count': reopened_bugs_count,
                    'status_counts': status_counts,
                    'issue_types': issue_types,
                    'recent_issues': recent_issues,
                    'participants': participants,
                    'total_participants': len(participants) if participants else 0,
                    # New statistics
                    'reopeners': sorted_reopeners,
                    'assignee_bug_stats': sorted_assignee_bug_stats
                }
                
                # If participant filter is provided, filter the results in memory
                if participant_filter:
                    logger.info(f"Filtering statistics for participant: {participant_filter}")
                    statistics = self._filter_statistics_by_participant(statistics, participant_filter)

                logger.info(
                    f"Generated statistics for project {project_key}: {bugs_count} bugs, {reopened_bugs_count} reopened bugs")
                self.core._projects_cache[cache_key] = statistics
                return statistics
            except Exception as e:
                logger.error(f"Error creating or caching statistics for project {project_key}: {str(e)}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")

                # Return a minimal valid statistics object
                minimal_stats = {
                    'total_issues': len(all_issues) if all_issues else 0,
                    'completed_tasks_count': completed_tasks_count,
                    'bugs_count': bugs_count,
                    'reopened_bugs_count': reopened_bugs_count,
                    'status_counts': status_counts or {},
                    'issue_types': issue_types or {},
                    'recent_issues': [],
                    'participants': [],
                    'total_participants': 0,
                    'reopeners': sorted_reopeners or [],
                    'assignee_bug_stats': sorted_assignees or []
                }
                self.core._projects_cache[cache_key] = minimal_stats
                return minimal_stats

        except Exception as e:
            logger.error(f"Error generating project statistics: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {}

    def test_get_status_transitions(self, project_key, limit=10):
        """
        Test function to get actual status transitions from a project's bugs
        This helps identify the real status names and flow in your Jira instance
        """
        if not self.core.is_configured():
            logger.error("Jira API credentials not configured")
            return {}

        try:
            # Get bugs in the project
            from jira.issues import IssueHandler
            issue_handler = IssueHandler(self.core)
            jql = f"project = {project_key} AND issuetype = Bug ORDER BY updated DESC"
            bugs = issue_handler.search_issues(
                jql,
                fields="key,summary,status",
                max_results=limit,
                use_cache=False
            )

            if not bugs:
                logger.info(f"No bugs found in project {project_key}")
                return {}

            # Get all available statuses
            all_statuses = self.core.get_available_statuses()
            status_names = {status["id"]: status["name"] for status in all_statuses}

            # Store transitions we find
            transitions = {}
            transitions_count = 0

            # For each bug, analyze status changes
            for bug in bugs:
                issue_key = bug.get('key')
                current_status = bug.get('fields', {}).get('status', {}).get('name', 'Unknown')

                # Get issue with changelog
                issue_detail = issue_handler.get_issue_with_changelog(issue_key)
                if not issue_detail:
                    continue

                changelog = issue_detail.get('changelog', {}).get('histories', [])

                logger.info(f"Analyzing status transitions for bug {issue_key} (current: {current_status})")

                # Process status changes
                for history in changelog:
                    for item in history.get('items', []):
                        if item.get('field') == 'status':
                            from_status = item.get('fromString', '(unknown)')
                            to_status = item.get('toString', '(unknown)')

                            # Log this transition
                            transition_key = f"{from_status} → {to_status}"
                            if transition_key not in transitions:
                                transitions[transition_key] = 0
                            transitions[transition_key] += 1
                            transitions_count += 1

                            logger.info(f"  {issue_key}: {from_status} → {to_status}")

            logger.info(f"Found {transitions_count} status transitions across {len(bugs)} bugs")
            # Sort transitions by frequency
            sorted_transitions = sorted(
                [(k, v) for k, v in transitions.items()],
                key=lambda x: x[1],
                reverse=True
            )

            # Determine what transitions might indicate a reopening
            reopen_candidates = []
            for transition, count in sorted_transitions:
                from_status, to_status = transition.split(' → ')
                # Look for patterns that might indicate reopening
                if any(review in from_status.lower() for review in
                       ["review", "resolved", "done", "closed", "completed"]) and any(
                        early in to_status.lower() for early in
                        ["open", "progress", "todo", "to do", "new", "backlog"]):
                    reopen_candidates.append((transition, count))

            result = {
                "all_statuses": [status["name"] for status in all_statuses],
                "transitions_found": sorted_transitions,
                "possible_reopen_transitions": reopen_candidates
            }

            return result

        logger.info(f"Found {len(all_issues)} bugs in project {project_key}, processing...")

        # Extract relevant fields from the issues
        all_issues = [issue_handler._extract_fields(issue) for issue in all_issues]

        # Get all available statuses
        all_statuses = self.core.get_available_statuses()
        status_names = {status["id"]: status["name"] for status in all_statuses}

            # OPTIMIZATION: Get bugs with changelog in a single request
            # This reduces API calls by using the expand parameter
            from jira.issues import IssueHandler
            issue_handler = IssueHandler(self.core)
            all_bugs = issue_handler.search_issues(
                jql,
                fields="key,summary,status,assignee,reporter,issuetype,priority,created,updated",
                expand="changelog",  # Include changelog directly to avoid separate API calls
                max_results=200,
                use_cache=False
            )

            if not all_bugs:
                logger.info(f"No bugs found in project {project_key}")
                self.core._issues_cache[cache_key] = []
                return []

            logger.info(f"Found {len(all_bugs)} bugs in project {project_key}, checking for reopens...")

            # For each bug, check its changelog to see if it was reopened
            reopened_bugs = []

            # Define status transition patterns that indicate reopening
            from_states = ["reviewing", "review", "in review", "under review", "done", "closed"]
            to_states = ["todo", "to do", "in progress", "reopened", "request", "backlog", "open"]

            for bug in all_bugs:
                if not bug:
                    continue  # Skip None values

                issue_key = bug.get('key')
                changelog = bug.get('changelog', {}).get('histories', [])  # Get changelog directly from expanded data

                # Look for status changes that indicate a reopen
                was_reopened = False
                reopen_time = None
                from_status_value = ""
                to_status_value = ""
                reopen_by = ""  # Who made the reopen action

                for history in changelog:
                    for item in history.get('items', []):
                        if item.get('field') == 'status':
                            from_status = item.get('fromString', '').lower()
                            to_status = item.get('toString', '').lower()

                            # Check if this transition matches our definition of "reopened"
                            # That is, moving from a reviewing status back to an earlier status
                            # Using the CHANGED FROM ... TO logic as per JIRA JQL syntax
                            if any(review_status in from_status for review_status in from_states) and \
                                    any(early_status in to_status for early_status in to_states):
                                was_reopened = True
                                reopen_time = history.get('created')
                                from_status_value = item.get('fromString', '')
                                to_status_value = item.get('toString', '')

                                # Get who performed the status change (reopen action)
                                if 'author' in history:
                                    reopen_by = history.get('author', {}).get('displayName', 'Unknown')

                                logger.info(
                                    f"Found reopened bug {issue_key}: {from_status} → {to_status} by {reopen_by}")
                                break

                    if was_reopened:
                        break

                if was_reopened:
                    bug['was_reopened'] = True
                    bug['reopen_time'] = reopen_time
                    bug['reopen_from'] = from_status_value
                    bug['reopen_to'] = to_status_value
                    bug['reopen_by'] = reopen_by  # Store who reopened it
                    reopened_bugs.append(bug)

            logger.info(f"Found {len(reopened_bugs)} reopened bugs out of {len(all_bugs)} bugs")

            # Cache the result
            self.core._issues_cache[cache_key] = reopened_bugs
            return reopened_bugs

        except Exception as e:
            logger.error(f"Error searching for reopened bugs: {str(e)}")
            self.core._issues_cache[cache_key] = []
            return []

    def find_reopened_bugs(self, project_keys):
        """
        Find bugs that have been reopened across multiple projects
        """
        if not isinstance(project_keys, list):
            project_keys = [project_keys]

        all_reopened_bugs = []
        for project_key in project_keys:
            logger.info(f"Checking project {project_key} for reopened bugs")
            project_reopened_bugs = self.find_reopened_bugs_by_jql(project_key)
            all_reopened_bugs.extend(project_reopened_bugs)

        logger.info(f"Total reopened bugs across all projects: {len(all_reopened_bugs)}")
        return all_reopened_bugs
